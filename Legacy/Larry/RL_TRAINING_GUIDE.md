# ğŸ¤– **ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í›ˆë ¨ ê°€ì´ë“œ**

> **ë‹´ë‹¹**: Chloe Lee (cl4490)  
> **ëª©í‘œ**: íœ´ë¦¬ìŠ¤í‹± AIë¥¼ ë„˜ì–´ì„œëŠ” í•™ìŠµëœ RL ì—ì´ì „íŠ¸ ê°œë°œ

---

## ğŸ“‹ **í˜„ì¬ ìƒíƒœ**

### **1. AI ëª¨ë“œ êµ¬í˜„ ì™„ë£Œ âœ…**

- íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ AI ì—ì´ì „íŠ¸ ë™ì‘
- ë©”í…Œì˜¤ íšŒí”¼ & ë³„ ìˆ˜ì§‘ ë¡œì§
- ì¤‘ì•™ ìœ ì§€ ì „ëµ

### **2. RL ì¤€ë¹„ ì™„ë£Œ âœ…**

- ìƒíƒœ ì¸ì½”ë”© í•¨ìˆ˜: `encode_game_state()` (10ì°¨ì› ë²¡í„°)
- ì•¡ì…˜ ê³µê°„: `['stay', 'left', 'right', 'jump']`
- ë³´ìƒ ì‹œìŠ¤í…œ: ìƒì¡´(+1), ë³„ íšë“(+20), ë©”í…Œì˜¤ ì¶©ëŒ(-100)

### **3. ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ âœ…**

- `web_app/data/gameplay_sessions/`: Human & AI í”Œë ˆì´ ë°ì´í„°
- `web_app/collected_gameplay/`: State-Action-Reward ë¡œê·¸ (JSONL)

---

## ğŸ¯ **RL í›ˆë ¨ ëª©í‘œ**

### **Phase 1: Imitation Learning (Policy Distillation)**

í˜„ì¬ íœ´ë¦¬ìŠ¤í‹± AIì˜ í–‰ë™ì„ ëª¨ë°©í•˜ëŠ” ì •ì±… ë„¤íŠ¸ì›Œí¬ í›ˆë ¨

### **Phase 2: Reinforcement Learning (PPO/DQN)**

ìê¸° ê²½ê¸°ë¥¼ í†µí•´ íœ´ë¦¬ìŠ¤í‹± AIë¥¼ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ ë‹¬ì„±

---

## ğŸ—ï¸ **RL ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜**

### **ìƒíƒœ ê³µê°„ (State Space)**

`encode_game_state(game)` í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” **10ì°¨ì› ë²¡í„°**:

```python
state = [
    player_x_normalized,          # [0, 1]
    player_y_normalized,          # [0, 1]
    player_vy_normalized,         # [-1, 1]
    nearest_meteor_dx,            # [-1, 1] (ìƒëŒ€ x ê±°ë¦¬)
    nearest_meteor_dy,            # [0, 1] (ìƒëŒ€ y ê±°ë¦¬)
    nearest_meteor_distance,      # [0, 1] (ìœ í´ë¦¬ë“œ ê±°ë¦¬)
    nearest_star_dx,              # [-1, 1]
    nearest_star_dy,              # [0, 1]
    nearest_star_distance,        # [0, 1]
    on_ground                     # {0, 1} (ë°”ë‹¥ ì ‘ì´‰ ì—¬ë¶€)
]
```

### **ì•¡ì…˜ ê³µê°„ (Action Space)**

4ê°€ì§€ ì´ì‚° ì•¡ì…˜:

```python
actions = ['stay', 'left', 'right', 'jump']
# stay: ì•„ë¬´ í–‰ë™ë„ í•˜ì§€ ì•ŠìŒ
# left: ì™¼ìª½ ì´ë™ (-10 pixels)
# right: ì˜¤ë¥¸ìª½ ì´ë™ (+10 pixels)
# jump: ì í”„ (vy = -18)
```

### **ë³´ìƒ í•¨ìˆ˜ (Reward Function)**

```python
# ê¸°ë³¸ ë³´ìƒ
reward = 1.0  # ë§¤ í”„ë ˆì„ ìƒì¡´

# ì´ë²¤íŠ¸ ë³´ìƒ
if cleared_obstacles > 0:
    reward += cleared_obstacles * 5  # ì¥ì• ë¬¼ íšŒí”¼ ì„±ê³µ

if star_collected:
    reward += 20  # ë³„ íšë“ (OBJECT_TYPES['star']['reward'])

if game_over:
    reward = -100  # ë©”í…Œì˜¤ ì¶©ëŒ (OBJECT_TYPES['meteor']['reward'])
```

---

## ğŸš€ **êµ¬í˜„ ë‹¨ê³„**

### **Step 1: ë°ì´í„° ì¤€ë¹„**

#### **í›ˆë ¨ ë°ì´í„° ë¡œë“œ**

```python
import json
from pathlib import Path

def load_training_data():
    """ìˆ˜ì§‘ëœ gameplay ë°ì´í„° ë¡œë“œ"""
    data_dir = Path("web_app/collected_gameplay")
    episodes = []

    for session_dir in data_dir.iterdir():
        if session_dir.is_dir():
            states_file = session_dir / "states_actions.jsonl"

            episode = []
            with open(states_file, 'r') as f:
                for line in f:
                    record = json.loads(line)
                    episode.append({
                        'state': record['state'],
                        'action': record['action'],
                        'reward': record['reward'],
                        'done': record['done']
                    })

            episodes.append(episode)

    return episodes
```

### **Step 2: ì •ì±… ë„¤íŠ¸ì›Œí¬ ì •ì˜**

```python
import torch
import torch.nn as nn

class PolicyNetwork(nn.Module):
    """ì •ì±… ë„¤íŠ¸ì›Œí¬ (MLP)"""
    def __init__(self, state_dim=10, hidden_dim=128, action_dim=4):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )

    def forward(self, state):
        return self.network(state)
```

### **Step 3: Imitation Learning (ì§€ë„í•™ìŠµ)**

```python
def train_imitation(model, episodes, epochs=100):
    """íœ´ë¦¬ìŠ¤í‹± AI ëª¨ë°© í•™ìŠµ"""
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    action_map = {'stay': 0, 'move_left': 1, 'move_right': 2, 'jump': 3}

    for epoch in range(epochs):
        total_loss = 0

        for episode in episodes:
            for step in episode:
                # ìƒíƒœ ì¸ì½”ë”©
                state = encode_state_from_record(step['state'])
                state_tensor = torch.FloatTensor(state).unsqueeze(0)

                # ì•¡ì…˜ ë ˆì´ë¸”
                action = step['action']
                action_idx = action_map.get(action, 0)
                action_tensor = torch.LongTensor([action_idx])

                # ìˆœì „íŒŒ
                action_probs = model(state_tensor)
                loss = criterion(action_probs, action_tensor)

                # ì—­ì „íŒŒ
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}")

    return model
```

### **Step 4: PPO ê°•í™”í•™ìŠµ**

```python
from torch.distributions import Categorical

class PPOAgent:
    """PPO ì—ì´ì „íŠ¸"""
    def __init__(self, state_dim=10, action_dim=4, lr=3e-4):
        self.policy = PolicyNetwork(state_dim, 128, action_dim)
        self.value_net = ValueNetwork(state_dim, 128)
        self.optimizer = torch.optim.Adam(
            list(self.policy.parameters()) + list(self.value_net.parameters()),
            lr=lr
        )

    def select_action(self, state):
        """ì•¡ì…˜ ì„ íƒ (í™•ë¥ ì )"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy(state_tensor)
        dist = Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

    def update(self, trajectories, clip_epsilon=0.2, epochs=10):
        """PPO ì—…ë°ì´íŠ¸"""
        for _ in range(epochs):
            for traj in trajectories:
                states = torch.FloatTensor(traj['states'])
                actions = torch.LongTensor(traj['actions'])
                old_log_probs = torch.FloatTensor(traj['log_probs'])
                returns = torch.FloatTensor(traj['returns'])

                # í˜„ì¬ ì •ì±…ì˜ log prob
                action_probs = self.policy(states)
                dist = Categorical(action_probs)
                new_log_probs = dist.log_prob(actions)

                # Importance sampling ratio
                ratio = torch.exp(new_log_probs - old_log_probs)

                # Clipped surrogate objective
                advantages = returns - self.value_net(states).squeeze()
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages
                loss = -torch.min(surr1, surr2).mean()

                # ì—…ë°ì´íŠ¸
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
```

### **Step 5: ëª¨ë¸ í†µí•©**

```python
# ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), 'web_app/models/rl_agent.pth')

# app.pyì—ì„œ ë¡œë“œ
import torch
from models.policy_network import PolicyNetwork

RL_MODEL = PolicyNetwork(state_dim=10, action_dim=4)
RL_MODEL.load_state_dict(torch.load('models/rl_agent.pth'))
RL_MODEL.eval()
RL_MODEL_AVAILABLE = True
```

---

## ğŸ“Š **í‰ê°€ ì§€í‘œ**

### **ì„±ê³µ ê¸°ì¤€ (í”„ë¡œì íŠ¸ ì œì•ˆì„œ ê¸°ì¤€)**

1. **Imitation Accuracy**: â‰¥75% action agreement (íœ´ë¦¬ìŠ¤í‹± AI ëª¨ë°©)
2. **Self-Play Performance Gain**: +20% ìƒì¡´ ì‹œê°„ í–¥ìƒ
3. **Real-Time Inference**: â‰¤16.7 ms/frame (60 FPS)
4. **Absolute Benchmark**: í‰ê·  ìƒì¡´ ì‹œê°„ 119ì´ˆ

### **ì¸¡ì • ë°©ë²•**

```python
def evaluate_agent(model, num_episodes=100):
    """ì—ì´ì „íŠ¸ ì„±ëŠ¥ í‰ê°€"""
    survival_times = []
    scores = []

    for _ in range(num_episodes):
        # ê²Œì„ ì‹¤í–‰ (ì›¹ API í˜¸ì¶œ or ë¡œì»¬ ì‹œë®¬ë ˆì´ì…˜)
        result = play_game_with_agent(model)
        survival_times.append(result['time'])
        scores.append(result['score'])

    return {
        'avg_survival': np.mean(survival_times),
        'avg_score': np.mean(scores),
        'max_survival': np.max(survival_times),
        'std_survival': np.std(survival_times)
    }
```

---

## ğŸ› ï¸ **ë””ë²„ê¹… & ì‹œê°í™”**

### **TensorBoard ë¡œê¹…**

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/rl_training')

for episode in range(num_episodes):
    # í›ˆë ¨...
    writer.add_scalar('Loss/policy', policy_loss, episode)
    writer.add_scalar('Reward/episode', total_reward, episode)
    writer.add_scalar('Survival/time', survival_time, episode)

writer.close()
```

### **í•™ìŠµ ê³¡ì„  ì‹œê°í™”**

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(episode_rewards)
plt.title('Episode Reward')
plt.xlabel('Episode')
plt.ylabel('Total Reward')

plt.subplot(1, 3, 2)
plt.plot(survival_times)
plt.title('Survival Time')
plt.xlabel('Episode')
plt.ylabel('Time (s)')

plt.subplot(1, 3, 3)
plt.plot(losses)
plt.title('Training Loss')
plt.xlabel('Iteration')
plt.ylabel('Loss')

plt.tight_layout()
plt.savefig('training_progress.png')
```

---

## ğŸ”— **ì°¸ê³  ìë£Œ**

### **ë…¼ë¬¸**

- [Proximal Policy Optimization (Schulman et al., 2017)](https://arxiv.org/abs/1707.06347)
- [Deep Q-Network (Mnih et al., 2015)](https://www.nature.com/articles/nature14236)

### **ì½”ë“œ ì˜ˆì œ**

- [OpenAI Spinning Up: PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)

### **í”„ë¡œì íŠ¸ ë‚´ ì°¸ê³  íŒŒì¼**

- `src/utils/rl_instrumentation.py`: RL ê³„ì¸¡ ìœ í‹¸ë¦¬í‹° (ë˜ë¦¬ ì‘ì„±)
- `web_app/app.py`: ê²Œì„ í™˜ê²½ & ë³´ìƒ í•¨ìˆ˜
- `DATA_STRATEGY.md`: ë°ì´í„° ì €ì¥ ì „ëµ

---

## ğŸ“ **í´ë¡œì—ê²Œ ì „ë‹¬ì‚¬í•­**

### **í˜„ì¬ ì¤€ë¹„ëœ ê²ƒ**

1. âœ… **ê²Œì„ í™˜ê²½**: Flask-SocketIO ê¸°ë°˜, ì™„ì „íˆ ë™ì‘
2. âœ… **ìƒíƒœ ì¸ì½”ë”©**: `encode_game_state()` í•¨ìˆ˜ (10ì°¨ì›)
3. âœ… **ë³´ìƒ í•¨ìˆ˜**: ìƒì¡´(+1), ë³„(+20), ì¶©ëŒ(-100)
4. âœ… **ë°ì´í„° ìˆ˜ì§‘**: ìˆ˜ë°± ê°œì˜ gameplay ì„¸ì…˜ (Human + AI)
5. âœ… **íœ´ë¦¬ìŠ¤í‹± AI**: ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ (ëª¨ë°© í•™ìŠµ íƒ€ê²Ÿ)

### **í´ë¡œê°€ í•  ì¼**

1. **Imitation Learning**: íœ´ë¦¬ìŠ¤í‹± AI ëª¨ë°© (â‰¥75% ì •í™•ë„)
2. **PPO/DQN í›ˆë ¨**: ìê¸° ê²½ê¸°ë¡œ ì„±ëŠ¥ í–¥ìƒ (+20% ìƒì¡´ ì‹œê°„)
3. **ëª¨ë¸ ìµœì í™”**: ONNX ë³€í™˜ í›„ ë˜ë¦¬ê°€ ë°°í¬
4. **ì‹¤í—˜ ì¶”ì **: TensorBoard/W&Bë¡œ í•™ìŠµ ê³¡ì„  ê¸°ë¡

### **ë‹¤ìŒ ë¯¸íŒ… ì „ê¹Œì§€**

- [ ] ê¸°ë³¸ ì •ì±… ë„¤íŠ¸ì›Œí¬ êµ¬í˜„ (PyTorch)
- [ ] Imitation Learning íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] ì²« ë²ˆì§¸ í•™ìŠµ ì‹¤í—˜ ì‹¤í–‰ & ê²°ê³¼ ê³µìœ 

---

**ğŸ“ ì‘ì„±ì**: Minsuk Kim (mk4434)  
**ğŸ“… ìµœì¢… ìˆ˜ì •**: 2025-11-18  
**ğŸ”— ê´€ë ¨ ë¬¸ì„œ**: `IMPLEMENTATION_ROADMAP.md`, `TEAM_BRIEFING.md`
